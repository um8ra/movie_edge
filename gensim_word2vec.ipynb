{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='word2vec.log', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = os.cpu_count() - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['ratings', 'trg', 'tst', 'val']>\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/40208420/how-to-find-hdf5-file-groups-keys-within-python\n",
    "with h5py.File('binarized.hdf') as f:\n",
    "    print(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLUMNS\n",
    "LIKED = 'Liked'\n",
    "MOVIE_ID = 'movieId'\n",
    "USER_ID = 'userId'\n",
    "TIMESTAMP = 'Timestamp'\n",
    "TITLE = 'title'\n",
    "GENRE = 'genres'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_df(_df):\n",
    "    _df.sort_values(by=[TIMESTAMP], inplace=True, ascending=True)\n",
    "    _df[MOVIE_ID] = _df.index.get_level_values(MOVIE_ID).astype(str)\n",
    "    return _df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = pd.read_csv('ml-20m/movies.csv', index_col=MOVIE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trg = pd.read_hdf('binarized.hdf', key='trg')\n",
    "df_trg = df_trg[df_trg[LIKED] == 1]\n",
    "# df_trg = df_trg.head(50000) # todo comment out for production\n",
    "df_trg = transform_df(df_trg)\n",
    "df_val = transform_df(pd.read_hdf('binarized.hdf', key='val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Liked</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>movieId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28507</th>\n",
       "      <th>1176</th>\n",
       "      <td>1</td>\n",
       "      <td>789652004</td>\n",
       "      <td>1176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">131160</th>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>789652009</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>789652009</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">99851</th>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>822873600</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1</td>\n",
       "      <td>822873600</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Liked  Timestamp movieId\n",
       "userId movieId                          \n",
       "28507  1176         1  789652004    1176\n",
       "131160 21           1  789652009      21\n",
       "       47           1  789652009      47\n",
       "99851  52           1  822873600      52\n",
       "       58           1  822873600      58"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trg_gb = df_trg.groupby([USER_ID])\n",
    "dict_groups_trg = {k: list(v[MOVIE_ID]) \n",
    "                   for k, v in df_trg_gb}\n",
    "MAX_WINDOW_SIZE = df_trg_gb[LIKED].count().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_gb = df_val.groupby([USER_ID])\n",
    "dict_groups_val = {k: list(v[MOVIE_ID]) \n",
    "                   for k, v in df_val_gb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_SIZE = 'vector_size'\n",
    "MIN_COUNT = 'min_count'\n",
    "WINDOW_SIZE = 'window_size'\n",
    "NEGATIVE_SAMPLING = 'negative_sampling'\n",
    "ITERATIONS = 'iterations'\n",
    "SKIP_GRAM = 'skip_gram'\n",
    "HIERARCHICAL_SOFTMAX = 'hierarchical_softmax'\n",
    "param_grid = ParameterGrid({\n",
    "    VECTOR_SIZE: [16, 24, 32],\n",
    "    MIN_COUNT: [1, 5, 10],\n",
    "    # todo, see if iterations makes much of a difference\n",
    "    ITERATIONS: [1],\n",
    "    WINDOW_SIZE: [MAX_WINDOW_SIZE, 32, 16],\n",
    "    NEGATIVE_SAMPLING: [2, 0],  # zero is no negative sampling\n",
    "    SKIP_GRAM: [1], # zero is no skip gram\n",
    "    HIERARCHICAL_SOFTMAX: [1, 0], # zero is no hierarchical softmax\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 1, 'negative_sampling': 2, 'skip_gram': 1, 'vector_size': 16, 'window_size': 5774}\n",
      "2019-10-18 16:52:00.409894\n",
      "2019-10-18 17:25:31.741200\n",
      "0 days 00:33:31.331306\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 1, 'negative_sampling': 2, 'skip_gram': 1, 'vector_size': 16, 'window_size': 32}\n",
      "2019-10-18 17:25:32.040229\n",
      "2019-10-18 17:26:35.393502\n",
      "0 days 00:01:03.353273\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 1, 'negative_sampling': 2, 'skip_gram': 1, 'vector_size': 16, 'window_size': 16}\n",
      "2019-10-18 17:26:35.710683\n",
      "2019-10-18 17:27:12.431084\n",
      "0 days 00:00:36.720401\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 1, 'negative_sampling': 2, 'skip_gram': 1, 'vector_size': 24, 'window_size': 5774}\n",
      "2019-10-18 17:27:13.139195\n",
      "2019-10-18 17:36:50.953453\n",
      "0 days 00:09:37.814258\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 1, 'negative_sampling': 2, 'skip_gram': 1, 'vector_size': 24, 'window_size': 32}\n",
      "2019-10-18 17:36:53.254526\n",
      "2019-10-18 17:49:16.706162\n",
      "0 days 00:12:23.451636\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 1, 'negative_sampling': 2, 'skip_gram': 1, 'vector_size': 24, 'window_size': 16}\n",
      "2019-10-18 17:49:17.040575\n",
      "2019-10-18 17:50:01.814205\n",
      "0 days 00:00:44.773630\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 1, 'negative_sampling': 2, 'skip_gram': 1, 'vector_size': 32, 'window_size': 5774}\n",
      "2019-10-18 17:50:02.136801\n",
      "2019-10-18 18:19:14.514736\n",
      "0 days 00:29:12.377935\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 1, 'negative_sampling': 2, 'skip_gram': 1, 'vector_size': 32, 'window_size': 32}\n",
      "2019-10-18 18:19:14.811403\n",
      "2019-10-18 18:24:13.172692\n",
      "0 days 00:04:58.361289\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 1, 'negative_sampling': 2, 'skip_gram': 1, 'vector_size': 32, 'window_size': 16}\n",
      "2019-10-18 18:24:13.461225\n",
      "2019-10-18 18:35:18.803957\n",
      "0 days 00:11:05.342732\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 1, 'negative_sampling': 0, 'skip_gram': 1, 'vector_size': 16, 'window_size': 5774}\n",
      "2019-10-18 18:35:19.119684\n",
      "2019-10-18 19:10:15.107468\n",
      "0 days 00:34:55.987784\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 1, 'negative_sampling': 0, 'skip_gram': 1, 'vector_size': 16, 'window_size': 32}\n",
      "2019-10-18 19:10:15.387544\n",
      "2019-10-18 19:11:05.272275\n",
      "0 days 00:00:49.884731\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 1, 'negative_sampling': 0, 'skip_gram': 1, 'vector_size': 16, 'window_size': 16}\n",
      "2019-10-18 19:11:05.560023\n",
      "2019-10-18 19:11:34.447698\n",
      "0 days 00:00:28.887675\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 1, 'negative_sampling': 0, 'skip_gram': 1, 'vector_size': 24, 'window_size': 5774}\n",
      "2019-10-18 19:11:35.146509\n",
      "2019-10-18 21:17:48.632555\n",
      "0 days 02:06:13.486046\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 1, 'negative_sampling': 0, 'skip_gram': 1, 'vector_size': 24, 'window_size': 32}\n",
      "2019-10-18 21:17:49.628733\n",
      "2019-10-18 21:39:54.084542\n",
      "0 days 00:22:04.455809\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 1, 'negative_sampling': 0, 'skip_gram': 1, 'vector_size': 24, 'window_size': 16}\n",
      "2019-10-18 21:39:54.348301\n",
      "2019-10-18 21:40:22.803912\n",
      "0 days 00:00:28.455611\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 1, 'negative_sampling': 0, 'skip_gram': 1, 'vector_size': 32, 'window_size': 5774}\n",
      "2019-10-18 21:40:23.048672\n",
      "2019-10-19 01:08:38.561386\n",
      "0 days 03:28:15.512714\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 1, 'negative_sampling': 0, 'skip_gram': 1, 'vector_size': 32, 'window_size': 32}\n",
      "2019-10-19 01:08:39.566865\n",
      "2019-10-19 01:10:46.816763\n",
      "0 days 00:02:07.249898\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 1, 'negative_sampling': 0, 'skip_gram': 1, 'vector_size': 32, 'window_size': 16}\n",
      "2019-10-19 01:10:47.836547\n",
      "2019-10-19 01:12:05.630568\n",
      "0 days 00:01:17.794021\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 5, 'negative_sampling': 2, 'skip_gram': 1, 'vector_size': 16, 'window_size': 5774}\n",
      "2019-10-19 01:12:06.634600\n",
      "2019-10-19 01:39:48.063969\n",
      "0 days 00:27:41.429369\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 5, 'negative_sampling': 2, 'skip_gram': 1, 'vector_size': 16, 'window_size': 32}\n",
      "2019-10-19 01:39:48.697640\n",
      "2019-10-19 01:43:30.830911\n",
      "0 days 00:03:42.133271\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 5, 'negative_sampling': 2, 'skip_gram': 1, 'vector_size': 16, 'window_size': 16}\n",
      "2019-10-19 01:43:31.444172\n",
      "2019-10-19 01:45:40.430101\n",
      "0 days 00:02:08.985929\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 5, 'negative_sampling': 2, 'skip_gram': 1, 'vector_size': 24, 'window_size': 5774}\n",
      "2019-10-19 01:45:41.041577\n",
      "2019-10-19 09:35:27.697077\n",
      "0 days 07:49:46.655500\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 5, 'negative_sampling': 2, 'skip_gram': 1, 'vector_size': 24, 'window_size': 32}\n",
      "2019-10-19 09:35:27.877916\n",
      "2019-10-19 09:36:44.879305\n",
      "0 days 00:01:17.001389\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 5, 'negative_sampling': 2, 'skip_gram': 1, 'vector_size': 24, 'window_size': 16}\n",
      "2019-10-19 09:36:45.076089\n",
      "2019-10-19 09:37:24.766000\n",
      "0 days 00:00:39.689911\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 5, 'negative_sampling': 2, 'skip_gram': 1, 'vector_size': 32, 'window_size': 5774}\n",
      "2019-10-19 09:37:24.940176\n",
      "2019-10-19 10:21:06.623129\n",
      "0 days 00:43:41.682953\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 5, 'negative_sampling': 2, 'skip_gram': 1, 'vector_size': 32, 'window_size': 32}\n",
      "2019-10-19 10:21:06.830906\n",
      "2019-10-19 10:21:59.419925\n",
      "0 days 00:00:52.589019\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 5, 'negative_sampling': 2, 'skip_gram': 1, 'vector_size': 32, 'window_size': 16}\n",
      "2019-10-19 10:21:59.656331\n",
      "2019-10-19 10:22:29.927745\n",
      "0 days 00:00:30.271414\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 5, 'negative_sampling': 0, 'skip_gram': 1, 'vector_size': 16, 'window_size': 5774}\n",
      "2019-10-19 10:22:30.151895\n",
      "2019-10-19 10:28:59.304405\n",
      "0 days 00:06:29.152510\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 5, 'negative_sampling': 0, 'skip_gram': 1, 'vector_size': 16, 'window_size': 32}\n",
      "2019-10-19 10:28:59.490058\n",
      "2019-10-19 10:29:50.309483\n",
      "0 days 00:00:50.819425\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 5, 'negative_sampling': 0, 'skip_gram': 1, 'vector_size': 16, 'window_size': 16}\n",
      "2019-10-19 10:29:50.492988\n",
      "2019-10-19 10:30:20.128082\n",
      "0 days 00:00:29.635094\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 5, 'negative_sampling': 0, 'skip_gram': 1, 'vector_size': 24, 'window_size': 5774}\n",
      "2019-10-19 10:30:20.319755\n",
      "2019-10-19 11:04:35.586547\n",
      "0 days 00:34:15.266792\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 5, 'negative_sampling': 0, 'skip_gram': 1, 'vector_size': 24, 'window_size': 32}\n",
      "2019-10-19 11:04:35.783474\n",
      "2019-10-19 11:05:38.275790\n",
      "0 days 00:01:02.492316\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 5, 'negative_sampling': 0, 'skip_gram': 1, 'vector_size': 24, 'window_size': 16}\n",
      "2019-10-19 11:05:38.467495\n",
      "2019-10-19 11:06:13.485043\n",
      "0 days 00:00:35.017548\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 5, 'negative_sampling': 0, 'skip_gram': 1, 'vector_size': 32, 'window_size': 5774}\n",
      "2019-10-19 11:06:13.685493\n",
      "2019-10-19 11:10:56.497300\n",
      "0 days 00:04:42.811807\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 5, 'negative_sampling': 0, 'skip_gram': 1, 'vector_size': 32, 'window_size': 32}\n",
      "2019-10-19 11:10:56.694019\n",
      "2019-10-19 11:11:34.356501\n",
      "0 days 00:00:37.662482\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 5, 'negative_sampling': 0, 'skip_gram': 1, 'vector_size': 32, 'window_size': 16}\n",
      "2019-10-19 11:11:34.565591\n",
      "2019-10-19 11:11:56.738313\n",
      "0 days 00:00:22.172722\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 10, 'negative_sampling': 2, 'skip_gram': 1, 'vector_size': 16, 'window_size': 5774}\n",
      "2019-10-19 11:11:56.958024\n"
     ]
    }
   ],
   "source": [
    "for params in param_grid:\n",
    "    print(params)\n",
    "    start_dttm = pd.Timestamp('now')\n",
    "    print(start_dttm)\n",
    "    logging.debug('Params: {params}'.format(params=params))\n",
    "    logging.debug('Start Train: {ts}'.format(ts=start_dttm))\n",
    "    \n",
    "    # Fit under grid parameters\n",
    "    model = Word2Vec(dict_groups_trg.values(),\n",
    "                     workers=workers,\n",
    "                     max_vocab_size=None,\n",
    "                     max_final_vocab=None,\n",
    "                     size=params[VECTOR_SIZE],\n",
    "                     sg=params[SKIP_GRAM],\n",
    "                     hs=params[HIERARCHICAL_SOFTMAX],\n",
    "                     min_count=params[MIN_COUNT],\n",
    "                     iter=params[ITERATIONS],\n",
    "                     window=params[WINDOW_SIZE],\n",
    "                     negative=params[NEGATIVE_SAMPLING],\n",
    "                     seed=42,\n",
    "                    )\n",
    "    # Reading the docs, we must still set PYTHONHASHSEED for reproducable runs\n",
    "    # So this helps... but not really\n",
    "    stop_dttm = pd.Timestamp('now')\n",
    "    print(stop_dttm)\n",
    "    logging.debug('Stop Train: {ts}'.format(ts=stop_dttm))\n",
    "    logging.debug('Params: {}'.format(params))\n",
    "    duration = stop_dttm - start_dttm\n",
    "    logging.debug('Duration: {}'.format(duration))\n",
    "    print(duration)\n",
    "    print('===\\n')\n",
    "    outpath = 'w2v_vs_{vs}_sg_{sg}_hs_{hs}_mc_{mc}_it_{it}_wn_{wn}_ng_{ng}.gensim'.format(\n",
    "        vs=params[VECTOR_SIZE], \n",
    "        sg=params[SKIP_GRAM],\n",
    "        hs=params[HIERARCHICAL_SOFTMAX],\n",
    "        mc=params[MIN_COUNT],\n",
    "        # lr=params[LEARNING_RATE],\n",
    "        it=params[ITERATIONS],\n",
    "        wn=params[WINDOW_SIZE], \n",
    "        ng=params[NEGATIVE_SAMPLING],\n",
    "    )\n",
    "    \n",
    "    if os.path.isfile(outpath):\n",
    "        os.remove(outpath)\n",
    "    model.save(outpath)\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_synonyms(search_str, num_synonyms):\n",
    "    synonym_list = list()\n",
    "    movie_index = df_movies[df_movies[TITLE].str.match(search_str)]\n",
    "    print(movie_index)\n",
    "    for mi in movie_index.index:\n",
    "        synonym_list.extend([(i, df_movies.loc[int(i[0])][TITLE]) for i in \n",
    "                             list(model.wv.most_similar(str(mi), topn=num_synonyms))])\n",
    "    return synonym_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('w2v_vs_128_mc_1_it_8_wn_5774_ng_5.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_synonyms('.*Matrix.*', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_synonyms('.*Private Ryan.*', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_synonyms('.*Star Wars: Episode.*', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
