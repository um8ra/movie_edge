{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='word2vec.log', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = os.cpu_count() - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['ratings', 'trg', 'tst', 'val']>\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/40208420/how-to-find-hdf5-file-groups-keys-within-python\n",
    "with h5py.File('binarized.hdf') as f:\n",
    "    print(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLUMNS\n",
    "LIKED = 'Liked'\n",
    "MOVIE_ID = 'movieId'\n",
    "USER_ID = 'userId'\n",
    "TIMESTAMP = 'Timestamp'\n",
    "TITLE = 'title'\n",
    "GENRE = 'genres'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_df(_df):\n",
    "    _df.sort_values(by=[TIMESTAMP], inplace=True, ascending=True)\n",
    "    _df[MOVIE_ID] = _df.index.get_level_values(MOVIE_ID).astype(str)\n",
    "    return _df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = pd.read_csv('ml-20m/movies.csv', index_col=MOVIE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trg = pd.read_hdf('binarized.hdf', key='trg')\n",
    "df_trg = df_trg[df_trg[LIKED] == 1]\n",
    "# df_trg = df_trg.head(50000) # todo comment out for production\n",
    "df_trg = transform_df(df_trg)\n",
    "df_val = transform_df(pd.read_hdf('binarized.hdf', key='val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Liked</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>movieId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28507</th>\n",
       "      <th>1176</th>\n",
       "      <td>1</td>\n",
       "      <td>789652004</td>\n",
       "      <td>1176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">131160</th>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>789652009</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>789652009</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">99851</th>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>822873600</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1</td>\n",
       "      <td>822873600</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Liked  Timestamp movieId\n",
       "userId movieId                          \n",
       "28507  1176         1  789652004    1176\n",
       "131160 21           1  789652009      21\n",
       "       47           1  789652009      47\n",
       "99851  52           1  822873600      52\n",
       "       58           1  822873600      58"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trg_gb = df_trg.groupby([USER_ID])\n",
    "dict_groups_trg = {k: list(v[MOVIE_ID]) \n",
    "                   for k, v in df_trg_gb}\n",
    "MAX_WINDOW_SIZE = df_trg_gb[LIKED].count().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_gb = df_val.groupby([USER_ID])\n",
    "dict_groups_val = {k: list(v[MOVIE_ID]) \n",
    "                   for k, v in df_val_gb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 'learning_rate'\n",
    "VECTOR_SIZE = 'vector_size'\n",
    "MIN_COUNT = 'min_count'\n",
    "WINDOW_SIZE = 'window_size'\n",
    "NEGATIVE_SAMPLING = 'negative_sampling'\n",
    "ITERATIONS = 'iterations'\n",
    "SKIP_GRAM = 'skip_gram'\n",
    "HIERARCHICAL_SOFTMAX = 'hierarchical_softmax'\n",
    "param_grid = ParameterGrid({\n",
    "#     LEARNING_RATE: [0.025, 0.01, 0.05],\n",
    "    VECTOR_SIZE: [64],\n",
    "    MIN_COUNT: [1],\n",
    "    ITERATIONS: [1],\n",
    "    WINDOW_SIZE: [MAX_WINDOW_SIZE],\n",
    "    NEGATIVE_SAMPLING: [0, 2],  # zero is no negative sampling\n",
    "    SKIP_GRAM: [1, 0], # zero is no skip gram\n",
    "    # todo HS = 0\n",
    "    HIERARCHICAL_SOFTMAX: [1, 0], # zero is no hierarchical softmax\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 1, 'negative_sampling': 0, 'skip_gram': 1, 'vector_size': 64, 'window_size': 5774}\n",
      "2019-10-17 20:58:45.883221\n",
      "2019-10-17 21:07:00.203197\n",
      "0 days 00:08:14.319976\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 1, 'negative_sampling': 0, 'skip_gram': 0, 'vector_size': 64, 'window_size': 5774}\n",
      "2019-10-17 21:07:00.509775\n",
      "2019-10-17 21:07:31.688483\n",
      "0 days 00:00:31.178708\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 1, 'negative_sampling': 2, 'skip_gram': 1, 'vector_size': 64, 'window_size': 5774}\n",
      "2019-10-17 21:07:32.000162\n",
      "2019-10-17 21:14:13.631260\n",
      "0 days 00:06:41.631098\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 1, 'iterations': 1, 'min_count': 1, 'negative_sampling': 2, 'skip_gram': 0, 'vector_size': 64, 'window_size': 5774}\n",
      "2019-10-17 21:14:13.993275\n",
      "2019-10-17 21:15:15.660056\n",
      "0 days 00:01:01.666781\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 0, 'iterations': 1, 'min_count': 1, 'negative_sampling': 0, 'skip_gram': 1, 'vector_size': 64, 'window_size': 5774}\n",
      "2019-10-17 21:15:16.009615\n",
      "2019-10-17 21:15:21.561438\n",
      "0 days 00:00:05.551823\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 0, 'iterations': 1, 'min_count': 1, 'negative_sampling': 0, 'skip_gram': 0, 'vector_size': 64, 'window_size': 5774}\n",
      "2019-10-17 21:15:21.640880\n",
      "2019-10-17 21:15:27.122447\n",
      "0 days 00:00:05.481567\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 0, 'iterations': 1, 'min_count': 1, 'negative_sampling': 2, 'skip_gram': 1, 'vector_size': 64, 'window_size': 5774}\n",
      "2019-10-17 21:15:27.198033\n",
      "2019-10-17 21:17:24.734389\n",
      "0 days 00:01:57.536356\n",
      "===\n",
      "\n",
      "{'hierarchical_softmax': 0, 'iterations': 1, 'min_count': 1, 'negative_sampling': 2, 'skip_gram': 0, 'vector_size': 64, 'window_size': 5774}\n",
      "2019-10-17 21:17:24.836106\n",
      "2019-10-17 21:17:55.433473\n",
      "0 days 00:00:30.597367\n",
      "===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for params in param_grid:\n",
    "    print(params)\n",
    "    start_dttm = pd.Timestamp('now')\n",
    "    print(start_dttm)\n",
    "    logging.debug('Params: {params}'.format(params=params))\n",
    "    logging.debug('Start Train: {ts}'.format(ts=start_dttm))\n",
    "    \n",
    "    # Fit under grid parameters\n",
    "    model = Word2Vec(dict_groups_trg.values(),\n",
    "                     workers=workers,\n",
    "                     max_vocab_size=None,\n",
    "                     max_final_vocab=None,\n",
    "                     size=params[VECTOR_SIZE],\n",
    "                     sg=params[SKIP_GRAM],\n",
    "                     hs=params[HIERARCHICAL_SOFTMAX],\n",
    "                     min_count=params[MIN_COUNT],\n",
    "                     # alpha=params[LEARNING_RATE],\n",
    "                     iter=params[ITERATIONS],\n",
    "                     window=params[WINDOW_SIZE],\n",
    "                     negative=params[NEGATIVE_SAMPLING],\n",
    "                     seed=42,\n",
    "                    )\n",
    "    # Reading the docs, we must still set PYTHONHASHSEED for reproducable runs\n",
    "    # So this helps... but not really\n",
    "    stop_dttm = pd.Timestamp('now')\n",
    "    print(stop_dttm)\n",
    "    logging.debug('Stop Train: {ts}'.format(ts=stop_dttm))\n",
    "    logging.debug('Params: {}'.format(params))\n",
    "    duration = stop_dttm - start_dttm\n",
    "    logging.debug('Duration: {}'.format(duration))\n",
    "    print(duration)\n",
    "    print('===\\n')\n",
    "    outpath = 'w2v_vs_{vs}_sg_{sg}_hs_{hs}_mc_{mc}_it_{it}_wn_{wn}_ng_{ng}.gensim'.format(\n",
    "        vs=params[VECTOR_SIZE], \n",
    "        sg=params[SKIP_GRAM],\n",
    "        hs=params[HIERARCHICAL_SOFTMAX],\n",
    "        mc=params[MIN_COUNT],\n",
    "        # lr=params[LEARNING_RATE],\n",
    "        it=params[ITERATIONS],\n",
    "        wn=params[WINDOW_SIZE], \n",
    "        ng=params[NEGATIVE_SAMPLING],\n",
    "    )\n",
    "    \n",
    "    if os.path.isfile(outpath):\n",
    "        os.remove(outpath)\n",
    "    model.save(outpath)\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_synonyms(search_str, num_synonyms):\n",
    "    synonym_list = list()\n",
    "    movie_index = df_movies[df_movies[TITLE].str.match(search_str)]\n",
    "    print(movie_index)\n",
    "    for mi in movie_index.index:\n",
    "        synonym_list.extend([(i, df_movies.loc[int(i[0])][TITLE]) for i in \n",
    "                             list(model.wv.most_similar(str(mi), topn=num_synonyms))])\n",
    "    return synonym_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('w2v_vs_128_mc_1_it_8_wn_5774_ng_5.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_synonyms('.*Matrix.*', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_synonyms('.*Private Ryan.*', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_synonyms('.*Star Wars: Episode.*', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
