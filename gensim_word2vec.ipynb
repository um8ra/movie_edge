{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='word2vec.log', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = os.cpu_count() - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/40208420/how-to-find-hdf5-file-groups-keys-within-python\n",
    "with h5py.File('binarized.hdf') as f:\n",
    "    print(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLUMNS\n",
    "LIKED = 'Liked'\n",
    "MOVIE_ID = 'movieId'\n",
    "USER_ID = 'userId'\n",
    "TIMESTAMP = 'Timestamp'\n",
    "TITLE = 'title'\n",
    "GENRE = 'genres'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_df(_df):\n",
    "    _df.sort_values(by=[TIMESTAMP], inplace=True, ascending=True)\n",
    "    _df[MOVIE_ID] = _df.index.get_level_values(MOVIE_ID).astype(str)\n",
    "    return _df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = pd.read_csv('ml-20m/movies.csv', index_col=MOVIE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trg = pd.read_hdf('binarized.hdf', key='trg')\n",
    "df_trg = df_trg[df_trg[LIKED] == 1]\n",
    "# df_trg = df_trg.head(50000) # todo comment out for production\n",
    "df_trg = transform_df(df_trg)\n",
    "df_val = transform_df(pd.read_hdf('binarized.hdf', key='val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trg_gb = df_trg.groupby([USER_ID])\n",
    "dict_groups_trg = {k: list(v[MOVIE_ID]) \n",
    "                   for k, v in df_trg_gb}\n",
    "MAX_WINDOW_SIZE = df_trg_gb[LIKED].count().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_gb = df_val.groupby([USER_ID])\n",
    "dict_groups_val = {k: list(v[MOVIE_ID]) \n",
    "                   for k, v in df_val_gb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_SIZE = 'vector_size'\n",
    "MIN_COUNT = 'min_count'\n",
    "WINDOW_SIZE = 'window_size'\n",
    "NEGATIVE_SAMPLING = 'negative_sampling'\n",
    "ITERATIONS = 'iterations'\n",
    "SKIP_GRAM = 'skip_gram'\n",
    "HIERARCHICAL_SOFTMAX = 'hierarchical_softmax'\n",
    "param_grid = ParameterGrid({\n",
    "    VECTOR_SIZE: [16, 24, 32],\n",
    "    MIN_COUNT: [1, 5, 10],\n",
    "    # todo, see if iterations makes much of a difference\n",
    "    ITERATIONS: [1],\n",
    "    WINDOW_SIZE: [MAX_WINDOW_SIZE, 32, 16],\n",
    "    NEGATIVE_SAMPLING: [2, 0],  # zero is no negative sampling\n",
    "    SKIP_GRAM: [1], # zero is no skip gram\n",
    "    HIERARCHICAL_SOFTMAX: [1, 0], # zero is no hierarchical softmax\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in param_grid:\n",
    "    print(params)\n",
    "    start_dttm = pd.Timestamp('now')\n",
    "    print(start_dttm)\n",
    "    logging.debug('Params: {params}'.format(params=params))\n",
    "    logging.debug('Start Train: {ts}'.format(ts=start_dttm))\n",
    "    \n",
    "    # Fit under grid parameters\n",
    "    model = Word2Vec(dict_groups_trg.values(),\n",
    "                     workers=workers,\n",
    "                     max_vocab_size=None,\n",
    "                     max_final_vocab=None,\n",
    "                     size=params[VECTOR_SIZE],\n",
    "                     sg=params[SKIP_GRAM],\n",
    "                     hs=params[HIERARCHICAL_SOFTMAX],\n",
    "                     min_count=params[MIN_COUNT],\n",
    "                     iter=params[ITERATIONS],\n",
    "                     window=params[WINDOW_SIZE],\n",
    "                     negative=params[NEGATIVE_SAMPLING],\n",
    "                     seed=42,\n",
    "                    )\n",
    "    # Reading the docs, we must still set PYTHONHASHSEED for reproducable runs\n",
    "    # So this helps... but not really\n",
    "    stop_dttm = pd.Timestamp('now')\n",
    "    print(stop_dttm)\n",
    "    logging.debug('Stop Train: {ts}'.format(ts=stop_dttm))\n",
    "    logging.debug('Params: {}'.format(params))\n",
    "    duration = stop_dttm - start_dttm\n",
    "    logging.debug('Duration: {}'.format(duration))\n",
    "    print(duration)\n",
    "    print('===\\n')\n",
    "    outpath = 'w2v_vs_{vs}_sg_{sg}_hs_{hs}_mc_{mc}_it_{it}_wn_{wn}_ng_{ng}.gensim'.format(\n",
    "        vs=params[VECTOR_SIZE], \n",
    "        sg=params[SKIP_GRAM],\n",
    "        hs=params[HIERARCHICAL_SOFTMAX],\n",
    "        mc=params[MIN_COUNT],\n",
    "        # lr=params[LEARNING_RATE],\n",
    "        it=params[ITERATIONS],\n",
    "        wn=params[WINDOW_SIZE], \n",
    "        ng=params[NEGATIVE_SAMPLING],\n",
    "    )\n",
    "    \n",
    "    if os.path.isfile(outpath):\n",
    "        os.remove(outpath)\n",
    "    model.save(outpath)\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_synonyms(search_str, num_synonyms):\n",
    "    synonym_list = list()\n",
    "    movie_index = df_movies[df_movies[TITLE].str.match(search_str)]\n",
    "    print(movie_index)\n",
    "    for mi in movie_index.index:\n",
    "        synonym_list.extend([(i, df_movies.loc[int(i[0])][TITLE]) for i in \n",
    "                             list(model.wv.most_similar(str(mi), topn=num_synonyms))])\n",
    "    return synonym_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('w2v_vs_128_mc_1_it_8_wn_5774_ng_5.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_synonyms('.*Matrix.*', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_synonyms('.*Private Ryan.*', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_synonyms('.*Star Wars: Episode.*', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
