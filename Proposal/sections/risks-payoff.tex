\subsection{Risks and Payoffs?}

As discussed in the Weighted Word2Vec paper \cite{chang2017weighted}, Word2Vec is not the pinnacle of understanding embeddings, and there are other techniques that may prove fruitful. Historically, Word2Vec uses "One Hot Encoding" representation of input and output vectors. The main concept addressed by this paper is that proximity of words should be considered in Word2Vec and that the nodes of surrounding words may be purposefully weighted more or less to enable this. Another issue discussed in the paper with Word2Vec is the curse of dimensionality. Rather than having words, we have ratings for movies that can be 1-5 stars. If we were to make movieA-1star a "word", each movie has 11 options (0 to 5 by 0.5 step). We can apply the idea of proximity-based weighted Word2Vec to movies ratings (v.s. OHE for binary "thumbs up" or "thumbs down" data). However, this may cost us compute in that many modern speed improvements capitalize on the OHE nature of embeddings. Ideally, this approach will let us more accurately represent movie ratings in an embedding space. If the compute cost is too high (a risk) we may need to fall back to the OHE method with input feature space reduced to binary.

Risks include computational infeasibility and simply "biting off more than we can chew". Payoffs are that we aim to have a movie recommendation engine that allows us more detail and granularity than Netflix does, and certainly with more transparency as source data and source code are available. Previously, Netflix allowed people to feed data on a five star scale. They changed this to "thumbs up" or "thumbs down," a binary choice that increased ratings by 200\% \cite{Netflixh3:online}. However, we can use other movie ratings database to avoid their issue and introduce more granular user insights.
