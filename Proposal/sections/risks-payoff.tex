\subsection{Risks and Payoffs?}

As discussed in the Weighted Word2Vec paper [Chang 2017], Word2Vec is not the pinnacle of understanding embeddings and there are other techniques that may prove fruitful. The main concept addressed by this paper is that proximity of words should be considered in Word2Vec and that the neural network weights may be purposefully weighted more or less to enable this. Another issue discussed in the paper with Word2Vec is the curse of dimensionality. Rather than having words, we have ratings for movies that can be 1-5 stars. If we were to make movieA-1star a "word" this means that each movie has 11 options (0 to 5 by 0.5 step). This would drastically increase the size of the embedding space. Our approach, by not using a traditional one-hot embedding will allow us to keep dimensions low, as the Weighted Word2Vec paper addresses. However this may cost us compute in that many modern speed improvements capitalize on the 1-hot nature of embeddings. We hope this approach will let us more accurately represent movie ratings in an embedding space. If the compute cost is too high (a risk) we may need to fall back to the one-hot method with 11x feature space. Or round to the nearest star. survey

Risks include computational infeasibility and simply "biting off more than we can chew". Payoffs are that we aim to have a movie recommendation engine that allows us more detail and granularity than Netflix does, and certainly with more transparency as source data and source code are available. Previously, Netflix allowed people to feed data on a five star scale. In 2017, after deep learning started taking off (Tensorflow was released in 2015), they changed this to "thumbs up" or "thumbs down," a binary choice lacking granularity. We surmise that this is because neural network embeddings are based on one-hot encoded vectors and it was more efficient at Netflix's scale to utilize embeddings than a traditional feed-forward neural network.