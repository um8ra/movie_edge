\subsection{How much will it cost?}

As shown in the course syllabus, it seems we are getting some free cloud credits but not getting free time. Thus, we should optimize for time rather than money. As stated in \cite{Goyal2017} training with large datasets requires a lot of time. Increasing minibatch size can yield significant speedups, but often results in poor results. The main takeaway is the "Linear Scaling Rule: When the minibatch size is multiplied by k, multiply the learning rate by k." Our dataset scales at $O(n^2)$, so we can utilize this rule to increase our batch size (and learning rate) and have fewer backpropagation calls. Also addressed is when the neural network weights are changing significantly each epoch, primarily during early trainign. This is overcome by using less aggressive learning rates to start with. All of these are takeaways we can utilize in our project.

Another paper dealing with time based optimization is \cite{Akiba2017}. Similar to the Goyal paper this paper makes recommendations for improving training times. One of the more clever takeaways is the use of one optimizer for warmup (RMSprop) and a *different* optimizer later on (SGD). Similar to \cite{Goyal2017} they recommend a lower starting learning rate, then ramping up.

Overall, the main takeaways are that deep learning takes significant time and often, money. We intend to utilize the takeaways from these articles as well as utilize some intelligent sampling and filtering in our algorithm to improve training time. Discarding movies with few reviews would pass the muster of a stats professor (too low sample size), while simultaneously improving the training speed.