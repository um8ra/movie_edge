\subsection{Collaborative Filtering and Recommender Systems}

[$ \frac{1}{2} + \frac{1}{2} $ citation] Early CF systems were driven by user-based nearest-neighbor similarity methods \cite{herlocker1999algorithmic},\cite{smith2017two}. This is accomplished by weighting similarity between user vectors of ratings. While neighborhood-based CF are often easy to implement and explain, they are not as accurate as more complex models.

[$1$ citation] Item-based CF methods are driven by item-to-item similarity. Amazon constructed a similar-items table by finding items that people tend to buy together \cite{linden2003amazon}, \cite{smith2017two}. Real-time scalability is achieved via offline pre-processing. However, they focus on real-time query and forgo user-centric interpretability. 

[$1.5$ citations] The Netflix prize ushered in new CF algorithms based on matrix factorization (MF) \cite{funk2006netflix}. In MF, user-item ratings are viewed as a matrix with rows indexed by users and columns by items: ratings are the sparse matrix entries. MF seeks to estimate unknown ratings by approximating with a low-rank decomposition. MF is highly accurate for the movie rating prediction, and it remains the predominant CF technique today, especially after advances in accounting for implicit feedback and timing of ratings \cite{koren2008factorization}. MF methods will be a performance baseline for our RS.

[$1$ citation] One shortcoming of MF is that only user-item ratings are taken into account. Rendle \cite{rendle2012factorization} developed Factorization Machines (FM), a supervised learning algorithm which models feature interactions with factorized parameters.  FMs can be seen as a generalization of MF which can easily incorporate side-channel features in addition to the user-item ratings matrix for greater accuracy while being fast to train.

[1 + ? citations] Vector representation of words is widely adopted in Natural Language Processing (NLP) applications. Two Word2vec architectures, Continuous Bag-of-Words (CBOW) and Skim-gram models \cite{mikolov2013efficient},\cite{mikolov2013distributed}, \cite{rong2014word2vec} were introduced to learn embeddings from words in sentences. The embeddings capture contextual similarity. This can be applied to user-item interaction data \cite{ozsoy2016word}. Training Word2vec takes advantage of one-hot-encoding (OHE). However, movie ratings are often not binary, increasing architecture complexity.

\subsection{Interactive Recommendations}

[1 citation] Most RS are not interactive. \cite{schafer2002meta} introduced MetaLens, a system that affords users session-specific control over the recommendation process. This improved user satisfaction. MetaLens is limited to filtering or sorting the list of recommendations with user-specified criteria. The list of potential filters is hard-coded, and the filters reflect user constraints rather than preferences about the content. We believe that there should be more flexible ways to express user interests.

[1 citation] MovieExplorer \cite{taijala2018movieexplorer}, is a system in which users provide feedback to the RS, is closest in similarity to MovieEdge. MovieExplorer allows the user to navigate the latent factor space of a MF-based CF model by expressing their session-specific preferences for movies.  As the session progresses, MovieExplorer presents progressively better recommendations. MovieExplorer’s interactive exploration paradigm increased user satisfaction. However, MovieExplorer asks users to navigate the high dimensional latent factor space without a visual reference. MovieEdge will directly address this shortcoming using various visualization strategies.

\subsection{Visualization of Embedding Spaces}

[1.5 citation] T-Stochastic Neighbor Embedding \mbox{(T-SNE)} \cite{maaten2008visualizing} is a technique that projects high dimensional feature vectors into 2-3D space for visualization. \mbox{(T-SNE)} computes similarity scores between observations in the original high dimensional feature space and finds a low dimensional embedding such that points which are close in high dimensional space remain close in the projected space. \mbox{(T-SNE)} is nonlinear and the transformation is adaptive to the data distribution. \mbox{(T-SNE)} is known for producing high quality visualizations, albeit at the cost of hyperparameter tuning and high computational costs. Using \mbox{(T-SNE)} can be difficult, and much care must be taken with interpretation \cite{wattenberg2016how}.

[1 citation] Taking the gradient of the \mbox{(T-SNE)} cost function with respect to the coordinates in the embedding space can be compute expensive.  \cite{van2014accelerating} accelerates \mbox{(T-SNE)} through tree-based algorithms like the Barnes-Hut approximation to “summarize” observations.  

[1 citation] A similar tool to MovieEdge is Embedding Projector \cite{smilkov2016embedding} which leverages \mbox{(T-SNE)} to visualize high dimensional data. We will adopt this approach in the high dimensional taste space of our CF model but augment the approach by providing supplemental data and more user interactivity with the visualization. 

[1 citation] Our primary data set is MovieLens \cite{harper2016movielens}, which includes over 27,000 unique movies. We will employ Ward Hierarchical clustering \cite{ward1963hierarchical} to reduce the visual load. Its hierarchical nature will allow users to zoom in and out of the visualization as we dynamically expand and collapse the clusters.

\subsection{Evaluating Recommender Systems}

[1 citation] Traditionally, RS had been evaluated on accuracy. \cite{herlocker2004evaluating} proposed an alternative metric for RS evaluation: Serendipity/novelty, the tendency for the RS to produce  “interesting” recommendations. By visualizing the latent CF model space during exploration, MovieEdge greatly increases the chance of a serendipitous encounter.


