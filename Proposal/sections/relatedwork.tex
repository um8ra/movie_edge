\subsection{Collaborative Filtering Recommender Systems}

[$ \frac{1}{2} + \frac{1}{2} $ citation] Early CF systems were driven by user-based nearest-neighbor similarity methods \cite{herlocker1999algorithmic},\cite{smith2017two}, weighting similarity between user vectors of ratings. Neighborhood-based CF are often easy to implement and explain; they are not as accurate as more complex models.

[$1$ citation] Item-based CF methods are driven by item-to-item similarity. Amazon constructed a similar-items table by finding items that people tend to buy together \cite{linden2003amazon}, \cite{smith2017two}. Scalability is achieved via offline pre-processing. However, they focus on real-time query and forgo user-centric interpretability. 

[$1.5$ citations] The Netflix prize introduced new CF algorithms based on matrix factorization (MF) \cite{funk2006netflix}. User-item ratings are viewed as a matrix where rows represent users and columns movies: ratings are the sparse matrix entries. MF seeks to estimate unknown ratings by approximating with a low-rank decomposition. MF is highly accurate for movie rating prediction, and it remains the predominant CF technique today \cite{koren2008factorization}. MF methods will serve as a performance baseline.

[$1$ citation] One shortcoming of MF is that only user-item ratings are taken into account. Rendle \cite{rendle2012factorization} developed Factorization Machines (FM), a supervised learning algorithm which models feature interactions with factorized parameters.  FMs can be seen as a generalization of MF which can easily incorporate side-channel features in addition to the user-item ratings matrix for greater accuracy while being fast to train.

[1 + ? citations] Vector representation of words is widely adopted in Natural Language Processing (NLP). Two Word2vec architectures, Continuous Bag-of-Words (CBOW) and Skim-gram models \cite{mikolov2013efficient},\cite{mikolov2013distributed}, \cite{rong2014word2vec} were introduced to learn embeddings from words in sentences. The embeddings capture contextual similarity. This can be applied to user-item interaction data \cite{ozsoy2016word}. Training Word2vec takes advantage of one-hot-encoding (OHE). However, movie ratings are often not binary, increasing architecture complexity.

\subsection{Interactive Recommendations}

[1 citation] Most RS are not interactive. MetaLens \cite{schafer2002meta}gives users session-specific control over the recommendation process, improving user satisfaction. MetaLens is limited to filtering or sorting the list of recommendations with user-specified criteria. The list of potential filters is hard-coded and reflect user constraints rather than preferences about the content. 

[1 citation] MovieExplorer \cite{taijala2018movieexplorer}, which incorporates user feedback in the RS, is closest in similarity to MovieEdge. MovieExplorer allows the user to navigate the model's latent factor space by expressing their session-specific preferences for movies.  As the session progresses, MovieExplorer presents better recommendations. MovieExplorer’s interactive exploration paradigm increased user satisfaction. However, MovieExplorer asks users to navigate the high dimensional latent factor space without a visual reference. We will directly address this shortcoming using various visualization strategies.

\subsection{Visualization of Embedding Spaces}

[1.5 citation] T-Stochastic Neighbor Embedding \mbox{(T-SNE)} \cite{maaten2008visualizing} projects high dimensional feature vectors into 2-3D space for visualization. \mbox{(T-SNE)} computes similarity scores between observations in the original high dimensional feature space and finds a low dimensional embedding such that points which are close in high dimensional space remain close in the projected space. \mbox{(T-SNE)} is nonlinear and the transformation is adaptive to the data distribution. \mbox{(T-SNE)} is known for producing high quality visualizations, albeit at the cost of hyperparameter tuning and computational costs. Using \mbox{(T-SNE)} can be difficult, and much care must be taken with interpretation \cite{wattenberg2016how}.

[1 citation] Taking the gradient of the \mbox{(T-SNE)} cost function with respect to the coordinates in the embedding space can be compute expensive.  \cite{van2014accelerating} accelerates \mbox{(T-SNE)} through tree-based algorithms like the Barnes-Hut approximation to “summarize” observations.  

[1 citation] A similar tool to MovieEdge is Embedding Projector \cite{smilkov2016embedding} which leverages \mbox{(T-SNE)} to visualize high dimensional data. We will adopt this approach in the high dimensional taste space of our CF model, augmenting with supplemental data and user interactivity. 

[1 citation] Our primary dataset is MovieLens \cite{harper2016movielens}, which includes over 27,000 unique movies. Ward Hierarchical clustering \cite{ward1963hierarchical} will reduce the visual load:  its hierarchical nature will allow users to zoom in and out of the visualization as we dynamically expand and collapse the clusters.

\subsection{Evaluating Recommender Systems}

[1 citation] Traditionally, RS had been evaluated on accuracy. \cite{herlocker2004evaluating} proposed an alternative metric for RS evaluation: Serendipity/novelty, the tendency for the RS to produce  “interesting” recommendations. By visualizing the latent CF model space during exploration, MovieEdge greatly increases the chance of a serendipitous encounter.


