\subsection{Novelty}
What's new in your approach? Why will it be successful?

Recommendation engines are ubiquitous in the modern digital economy. However, they are typically designed to be unobtrusive and work behind the scenes. Even Netflix, with its famous investment in the 2007 Netflix prize, does not heavily advertise its engine when presenting recommended movies or shows. Most industrial recommender engines rely on relatively simple models. For example, while it adopted some novel approaches from the Netflix Prize, Netflix did not implement the \href{https://www.wired.com/2012/04/netflix-prize-costs/}{winning solution} Koren 2009 \cite{koren2009bellkor}, citing engineering costs. Similarly, Amazon continues to rely on a system rooted in item-to-item collaborative filtering \cite{smith2017two}, one of the simplest early approaches that scales by performing expensive calculations offline and focuses on finding similar items instead of similar customers \cite{linden2003amazon}. In the meantime, increasingly sophisticated recommendation algorithms have been developed in academia. For example, Koren 2008 \cite{koren2008factorization} extends the matrix factorization approach to movie recommendation pioneered \cite{funk2006netflix} in the Netflix Prize by combining matrix factorization with item-to-item collaborative filtering techniques, taking advantage of implicit feedback on non-rated movies inferred from user behavior. Rendle 2012 \cite{rendle2012factorization} developed the factorization machine, which generalizes previous matrix factorization techniqus, which were limited to 2nd order interactions, to higher order interactions while maintaining computational efficiency.

Ironically, while all the approaches described above are interpretable in some sense, most commercial implementations do not appear to leverage that interpretability to a large extent by allowing users to explicitly control how recommendations are being generated. This seems to be for two reasons. First, companies seem to want to keep the recommendation engine out of the limelight while surfacing the recommendations themselves. Second, matrix factorization recommendation produces latent factors that can be hard for humans to interpret, even if the total effect is a simple linear sum of components. That is to say, while the the recommendation is expressed in an easy to understand model, is embedded in a space that humans cannot easily interpret. We believe that not accounting for users' input at the recommendaton stage is a disservice - users should be able to tell the engine what they are looking for rather than having that information be imputed from their previous interactions with the system. 

Neural Network based embedding approaches have become increasingly of interest to the research community in recent years, owing to the widespread success of models like Word2Vec \cite{mikolov2013efficient}, \cite{rong2014word2vec}, and \cite{rong2014word2vec}. In Natural Language Processing applications, identifying the word context in an embedding vector space via representation learning has been widely adopted. To our knowledge, modern embedding techniques have not been applied to recommendation engines, although the now-traditional matrix factorization methods can be seen as a naive form of embedding, where activation functions are linear. We can treat movies as words and users as sentences to establish an embedding problem. One significant issue with embedding approaches is that the neural networks are highly parameterised and thus extremely difficult to interpret.

Recent work on LIME \cite{ribeiro2016model} \cite{ribeiro2016should} offers avenues to make any model interpretable, through the use of a locally faithful surrogate linear model. 

In this project, we believe we can use LIME and embedding to produce a recommendation engine that is at once accurate, easily interpretable, and highly visualizable around the local vicinity of user embedding. Importantly, this will allow us to finally "close the loop" on recommendation engines by using the LIME explanations as a mechanism for the user to guide the engine as it produces recommendations.
