{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tensorflow import keras as k  # using 2.0.0-rc0\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.sparse import dok_matrix, random, coo_matrix\n",
    "from collections import defaultdict, namedtuple, OrderedDict\n",
    "from torch import nn\n",
    "from torch.nn.functional import mse_loss\n",
    "import torch\n",
    "import pickle\n",
    "device_cpu = torch.device('cpu')\n",
    "device_cuda = torch.device('cuda:0')\n",
    "device = device_cpu if not torch.cuda.is_available() else device_cpu\n",
    "\n",
    "np.set_printoptions(linewidth=128)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "path = Path('ml-20m')  # First time using pathlib, pretty neat. \"division\" is cool\n",
    "sparseData = namedtuple('sparseData', \n",
    "                        ['rows_x_y', 'columns_x', 'values_x', 'columns_y', 'values_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "USERID = 'userId'\n",
    "MOVIEID = 'movieId'\n",
    "RATING = 'rating'\n",
    "TITLE = 'title'\n",
    "GENRES = 'genres'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "list(path.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df_ratings = pd.read_csv(path / 'ratings.csv', index_col=[USERID, MOVIEID])[RATING]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ratings.groupby([MOVIEID]).count().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df_ratings = df_ratings / 5.0  # Encodes the input between [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# WARNING: MOVIEID is not a 0 through X where X is len(df_movies)\n",
    "df_movies = pd.read_csv(path / 'movies.csv', index_col=MOVIEID)\n",
    "# That is why we're extracting the index to give us an ordering\n",
    "df_movies_index = df_movies.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "distinct_users = set(df_ratings.index.get_level_values(USERID))\n",
    "distinct_users_count = len(distinct_users)\n",
    "distinct_movies = set(df_movies.index.get_level_values(MOVIEID))\n",
    "distinct_movies_count = len(distinct_movies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "movie_id_decoder = {val: ix for ix, val in enumerate(df_movies_index)}\n",
    "user_id_decoder = {val: ix for ix, val in enumerate(distinct_users)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def series_to_nparray(_s):\n",
    "    # Needs to be list data structure since sparse vector assembly uses \"+\" operator\n",
    "    # Numpy would add whereas Python would merge the lists\n",
    "    _values = _s.to_numpy().tolist()\n",
    "    \n",
    "    # Needs to be list data structure since inner loop (vectorize) utilizes addition\n",
    "    # to do concatenation\n",
    "    # We also don't want to put this is the same array as the data since that will\n",
    "    # coerce our lovely indexes to floats, which can't be used as indexes\n",
    "    _index = [movie_id_decoder[i] for i in _s.index]\n",
    "    return (_index, _values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "dict_user_ratings = dict()\n",
    "for user_id in distinct_users:\n",
    "    user_data = df_ratings[user_id]\n",
    "    dict_user_ratings[user_id] = series_to_nparray(user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_input_output_vectorized(_user_ratings, sparse=True):\n",
    "    _len_user_ratings = len(_user_ratings[0])\n",
    "    _len_user_ratings_minus_one = _len_user_ratings - 1\n",
    "    _sparse_matrix_length = (_len_user_ratings * _len_user_ratings_minus_one)\n",
    "    \n",
    "    _zipped_data = zip(_user_ratings[0], _user_ratings[1])\n",
    "\n",
    "    _row_range_xy = list()\n",
    "    _col_range_x = list()\n",
    "    _col_range_y = list()\n",
    "    _values_x = list()\n",
    "    _values_y = list()\n",
    "\n",
    "    for _ix, (_ix_movie_index, _i_rating) in enumerate(_zipped_data):\n",
    "        _row_start = _ix * _len_user_ratings_minus_one\n",
    "        _row_stop  = (_ix + 1) * _len_user_ratings_minus_one\n",
    "        \n",
    "        _row_range_xy.extend(list(range(_row_start, _row_stop)))\n",
    "        \n",
    "        _col_range_x.extend([_ix_movie_index] * _len_user_ratings_minus_one)\n",
    "        _values_x.extend([_i_rating] * _len_user_ratings_minus_one)\n",
    "        \n",
    "        _col_range_y.extend(_user_ratings[0][0:_ix] + _user_ratings[0][_ix + 1:])\n",
    "        _values_y.extend(_user_ratings[1][0:_ix] + _user_ratings[1][_ix + 1:])\n",
    "    \n",
    "    if sparse:\n",
    "        _size_tensor = torch.Size([_sparse_matrix_length, distinct_movies_count])\n",
    "\n",
    "        _sparse_x_i = torch.LongTensor([_row_range_xy, _col_range_x]).to(device)\n",
    "        _sparse_x_vals = torch.FloatTensor(_values_x).to(device)\n",
    "\n",
    "        _sparse_matrix_x = torch.sparse.FloatTensor(\n",
    "            _sparse_x_i, _sparse_x_vals, _size_tensor).to(device)\n",
    "\n",
    "        _sparse_y_i = torch.LongTensor([_row_range_xy, _col_range_y]).to(device)\n",
    "        _sparse_y_vals = torch.FloatTensor(_values_y).to(device)\n",
    "\n",
    "        _sparse_matrix_y = torch.sparse.FloatTensor(\n",
    "            _sparse_y_i, _sparse_y_vals, _size_tensor).to(device)\n",
    "\n",
    "        _sparse_data = sparseData(_row_range_xy, _col_range_x, _values_x, _col_range_y, _values_y)\n",
    "    else:\n",
    "        _sparse_matrix_x = np.zeros((_len_user_ratings * (_len_user_ratings_minus_one), \n",
    "                                  distinct_movies_count))\n",
    "        _sparse_matrix_y = np.zeros((_len_user_ratings * (_len_user_ratings_minus_one), \n",
    "                                  distinct_movies_count))\n",
    "        _sparse_matrix_x[_row_range_xy, _col_range_x] = _values_x\n",
    "        _sparse_matrix_y[_row_range_xy, _col_range_y] = _values_y\n",
    "    \n",
    "    _sparse_data = sparseData(_row_range_xy, _col_range_x, \n",
    "                              _values_x, _col_range_y, _values_y)   \n",
    "    _sparse_data = None\n",
    "    return _sparse_matrix_x, _sparse_matrix_y, _sparse_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = form_input_output_vectorized(dict_user_ratings[1], sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = form_input_output_vectorized((dict_user_ratings[1][0][0:3], \n",
    "                                  dict_user_ratings[1][1][0:3]))\n",
    "print(x[0].to_dense())\n",
    "print(x[1].to_dense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# model = k.models.Sequential()\n",
    "# model.add(k.layers.Dense(128, activation='linear', input_dim=distinct_movies_count))\n",
    "# model.add(k.layers.Dense(distinct_movies_count, activation='linear'))\n",
    "# model.compile(loss='mse', optimizer=k.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good thing I did my RL assignment in both PyTorch and Keras :)\n",
    "# Keras was faster, but that was before version 1.0 of PyTorch\n",
    "learning_rate = 1e-4\n",
    "neurons = 128\n",
    "nn_config = OrderedDict([\n",
    "        ('in', nn.Linear(in_features=distinct_movies_count, out_features=neurons)),\n",
    "        ('ReLU1', nn.ReLU()),\n",
    "        ('H1', nn.Linear(in_features=neurons, out_features=neurons)),\n",
    "        ('ReLU2', nn.ReLU()),\n",
    "        ('out', nn.Linear(in_features=neurons, out_features=distinct_movies_count))\n",
    "    ])\n",
    "model = nn.Sequential(nn_config).to(device)\n",
    "\n",
    "# citation: https://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ix = 0\n",
    "\n",
    "sparse_data_dict = dict()\n",
    "\n",
    "for counter, user_id in enumerate(distinct_users):\n",
    "    user_ratings = dict_user_ratings[user_id]\n",
    "    tensorX, tensorY, sparse_data = form_input_output_vectorized(user_ratings, sparse=True)\n",
    "    \n",
    "    # Works with dense? Why not sparse?\n",
    "    tensorX = tensorX.to(device)\n",
    "    tensorY = tensorY.to(device)\n",
    "    \n",
    "    y_pred = model(tensorX).to(device)\n",
    "    print(y_pred)\n",
    "    \n",
    "    # Torch can't do operations on sparse + dense\n",
    "    # https://github.com/pytorch/pytorch/issues/2389\n",
    "    # Necessitating the conversion to dense\n",
    "    \n",
    "    loss = mse_loss(y_pred, tensorY.to_dense()).to(device) # issue is here\n",
    "    loss.to(device)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "    \n",
    "#     sparse_data_dict[user_id] = sparse_data # Store and write out to pickle?\n",
    "    \n",
    "    \n",
    "    if counter == 1000:\n",
    "        break\n",
    "    print('Counter: {} \\t User: {}'.format(counter, user_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('sparse_data.pickle', 'wb') as f:\n",
    "    pickle.dump(sparse_data_dict, f)\n",
    "\n",
    "\n",
    "\n",
    "# for user_id in random.shuffle(distinct_users):\n",
    "#     print(user_id)\n",
    "#     user_ratings = dict_user_ratings[user_id]\n",
    "#     \n",
    "#     batch_x, batch_y = form_input_output_vectorized_sparse(user_ratings, sparse=False)\n",
    "# #     data_x.append(batch_x)\n",
    "# #     data_y.append(batch_y)\n",
    "#     \n",
    "#     model.fit(x=batch_x, y=batch_y)\n",
    "#         \n",
    "#     if ix == 1000:\n",
    "#         break\n",
    "#     ix += 1\n",
    "    \n",
    "#     print(user)\n",
    "#     print(movie)\n",
    "#     print(row)\n",
    "#     print(movie_one_hot)\n",
    "#     print(output_vector)\n",
    "#     print(dict_cache)\n",
    "print('done')\n",
    "#     model.fit(movie_one_hot, output_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (cse6242_project)",
   "language": "python",
   "name": "pycharm-18400341"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
